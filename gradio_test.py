import gradio as gr
import shutil
# import cv2
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
from chains.local_doc_qa import LocalDocQA
import models.shared as shared
from models.loader.args import parser
from models.loader import LoaderCheckPoint
from configs.model_config import *
#*************************ä¸€äº›å…¨å±€å˜é‡*************************************#
local_doc_qa = LocalDocQA()
flag_csv_logger = gr.CSVLogger()
embedding_model_dict_list = list(embedding_model_dict.keys())
llm_model_dict_list = list(llm_model_dict.keys())
#################################CSSç±»å®šä¹‰#####################################
block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}
.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""
##################################################################################

##############################ä¸€äº›å±•ç¤ºç›¸å…³çš„ï¼ˆå­—ä½“å‚æ•°ï¼‰################################
default_theme_args = dict(
    font=["Source Sans Pro", 'ui-sans-serif', 'system-ui', 'sans-serif'],
    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],
)
#########################################################################################

###########################################ä¸€äº›æ ‡é¢˜æé†’###################################
webui_title = """
# mmEngine çŸ¥è¯†åº“é—®ç­”
ğŸ“•[https://mmengine.readthedocs.io/en/latest/notes/contributing.html)
"""
# default_vs = get_vs_list()[0] if len(get_vs_list()) > 1 else "ä¸ºç©º"
init_message = f"""ä½ å¥½ğŸ‘‹,æˆ‘æ˜¯åŸºäºLangchain+Internlmçš„mmEngineæ–‡æ¡£åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ–¹ä¾¿ä½ å¿«é€Ÿè·å–ç›¸å…³çš„æ–‡æ¡£ç»†èŠ‚ï¼Œæ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚ç…§ https://mmengine.readthedocs.io/en/latest/ 
"""
#########################################################################################
#****************************************************************************************#

#æµ‹è¯•å‡½æ•°ï¼ˆä¸æ˜¾ç¤ºï¼‰
def test_fn():
    return None


####
def get_answer(query, vs_path, history, mode, score_threshold=VECTOR_SEARCH_SCORE_THRESHOLD,
               vector_search_top_k=VECTOR_SEARCH_TOP_K, chunk_conent: bool = True,
               chunk_size=CHUNK_SIZE, streaming: bool = STREAMING):
    if mode == "mmEngineçŸ¥è¯†åº“é—®ç­”" and vs_path is not None and os.path.exists(vs_path) and "index.faiss" in os.listdir(vs_path):
        for resp, history in local_doc_qa.get_knowledge_based_answer(
                query=query, vs_path=vs_path, chat_history=history, streaming=streaming):
            source = "\n\n"
            source += "".join(
                [f"""<details> <summary>å‡ºå¤„ [{i + 1}] {os.path.split(doc.metadata["source"])[-1]}</summary>\n"""
                 f"""{doc.page_content}\n"""
                 f"""</details>"""
                 for i, doc in
                 enumerate(resp["source_documents"])])
            history[-1][-1] += source
            yield history, ""
    logger.info(f"flagging: username={FLAG_USER_NAME},query={query},vs_path={vs_path},mode={mode},history={history}")
    # flag_csv_logger.setup([query, vs_path, chatbot, mode], "flagged")
    flag_csv_logger.flag([query, vs_path, history, mode], username=FLAG_USER_NAME)

######################################å¤„ç†çŸ¥è¯†åº“ä½ç½®###################################################
def get_vs_path(vs_path='./knowledge_base'):
    os.makedirs(vs_path, exist_ok=True)
    return vs_path
#########################################################################################################

#####################################è·å–æ¨¡å‹(init_model)######################################################
def get_model():
    args = parser.parse_args()
    args_dict = vars(args)
    shared.loaderCheckPoint = LoaderCheckPoint(args_dict)
    llm_model_ins = shared.loaderLLM()
    try:
        local_doc_qa.init_cfg(llm_model=llm_model_ins)
        answer_result_stream_result = local_doc_qa.llm_model_chain(
            {"prompt": 
             "ä½ å¥½", 
             "history": [], "streaming": False})

        for answer_result in answer_result_stream_result['answer_result_stream']:
            print(answer_result.llm_output)
        reply = """æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"""
        logger.info(reply)
        return reply
    except Exception as e:
        logger.error(e)
        reply = """æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œè¯·åˆ°é¡µé¢å·¦ä¸Šè§’"æ¨¡å‹é…ç½®"é€‰é¡¹å¡ä¸­é‡æ–°é€‰æ‹©åç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®"""
        if str(e) == "Unknown platform: darwin":
            logger.info("è¯¥æŠ¥é”™å¯èƒ½å› ä¸ºæ‚¨ä½¿ç”¨çš„æ˜¯ macOS æ“ä½œç³»ç»Ÿï¼Œéœ€å…ˆä¸‹è½½æ¨¡å‹è‡³æœ¬åœ°åæ‰§è¡Œ Web UIï¼Œå…·ä½“æ–¹æ³•è¯·å‚è€ƒé¡¹ç›® README ä¸­æœ¬åœ°éƒ¨ç½²æ–¹æ³•åŠå¸¸è§é—®é¢˜ï¼š"
                        " https://github.com/imClumsyPanda/langchain-ChatGLM")
        else:
            logger.info(reply)
        return reply
#####################################################################################################

###################æ¨¡å¼æ›´å˜######################################################################
def change_mode(mode, history):
    if mode == "mmEngineçŸ¥è¯†åº“é—®ç­”":
        return gr.update(visible=True), gr.update(visible=False), history
        # + [[None, "ã€æ³¨æ„ã€‘ï¼šæ‚¨å·²è¿›å…¥çŸ¥è¯†åº“é—®ç­”æ¨¡å¼ï¼Œæ‚¨è¾“å…¥çš„ä»»ä½•æŸ¥è¯¢éƒ½å°†è¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢ï¼Œç„¶åä¼šè‡ªåŠ¨æ•´ç†çŸ¥è¯†åº“å…³è”å†…å®¹è¿›å…¥æ¨¡å‹æŸ¥è¯¢ï¼ï¼ï¼"]]
    else:
        return gr.update(visible=False), gr.update(visible=False), history
#####################################################################################################

###################å­˜å‚¨å‘é‡######################################################################
def get_vector_store(vs_id, files, sentence_size, history, one_conent, one_content_segmentation):
    vs_path = vs_id
    filelist = []
    if local_doc_qa.llm_model_chain and local_doc_qa.embeddings:
        if isinstance(files, list):
            # for file in files:
            #     filename = os.path.split(file)[-1]
            #     shutil.move(file, os.path.join(KB_ROOT_PATH, vs_id, "content", filename))
            #     filelist.append(os.path.join(KB_ROOT_PATH, vs_id, "content", filename))
            vs_path, loaded_files = local_doc_qa.init_knowledge_vector_store(filelist, vs_path, sentence_size)
        else:
            vs_path, loaded_files = local_doc_qa.one_knowledge_add(vs_path, files, one_conent, one_content_segmentation,
                                                                   sentence_size)
        if len(loaded_files):
            file_status = f"å·²æ·»åŠ  {'ã€'.join([os.path.split(i)[-1] for i in loaded_files if i])} å†…å®¹è‡³çŸ¥è¯†åº“ï¼Œå¹¶å·²åŠ è½½çŸ¥è¯†åº“ï¼Œè¯·å¼€å§‹æé—®"
        else:
            file_status = "æ–‡ä»¶æœªæˆåŠŸåŠ è½½ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶"
    else:
        file_status = "æ¨¡å‹æœªå®ŒæˆåŠ è½½ï¼Œè¯·å…ˆåœ¨åŠ è½½æ¨¡å‹åå†å¯¼å…¥æ–‡ä»¶"
        vs_path = None
    logger.info(file_status)
    return vs_path, None, history + [[None, file_status]], \
           gr.update(choices=local_doc_qa.list_file_from_vector_store(vs_path) if vs_path else [])
########################################################################################

with gr.Blocks(css=block_css, theme=gr.themes.Default(**default_theme_args)) as demo:
    # å®šä¹‰å˜é‡
    # è·å–çŸ¥è¯†åº“è·¯å¾„
    vs_path=gr.State(get_vs_path())
    # è·å–æ¨¡å‹
    model_status = gr.State(get_model())
    #å±•ç¤ºæ ‡é¢˜
    gr.Markdown(webui_title)
    # å¯¹è¯é€‰é¡¹å¡
    with gr.Tab("å¯¹è¯"):
        with gr.Row():
            with gr.Column(scale=10):
                # åˆå§‹åŒ– Chatbot
                chatbot = gr.Chatbot([[None, init_message], [None, model_status.value]],
                                     elem_id="chat-box",
                                     show_label=False).style(height=750)
                # æé—®è¾“å…¥æ¡†
                query = gr.Textbox(show_label=False,
                                   placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
            # with gr.Column(scale=5):
            #     # é€‰æ‹©å¯¹è¯æ¨¡å¼
            #     mode = gr.Radio(["ä¸Internlm7bå¯¹è¯", "mmEngineçŸ¥è¯†åº“é—®ç­”"],
            #                     label="è¯·é€‰æ‹©ä½¿ç”¨æ¨¡å¼",
            #                     value="mmEngineçŸ¥è¯†åº“é—®ç­”", )
            #     knowledge_set = gr.Accordion("çŸ¥è¯†åº“è®¾å®š", visible=False)
            #     vs_setting = gr.Accordion("é…ç½®çŸ¥è¯†åº“")
            #     mode.change(fn=change_mode,
            #                 inputs=[mode, chatbot],
            #                 outputs=[vs_setting, knowledge_set, chatbot])
            #     with vs_setting:
            #         # æ›´æ–°å·²æœ‰çŸ¥è¯†åº“é€‰é¡¹
            #         vs_refresh = gr.Button("æ›´æ–°å·²æœ‰çŸ¥è¯†åº“é€‰é¡¹")
            
                one_title = gr.Textbox(label="æ ‡é¢˜", placeholder="è¯·è¾“å…¥è¦æ·»åŠ å•æ¡æ®µè½çš„æ ‡é¢˜", lines=1)
                one_conent = gr.Textbox(label="å†…å®¹", placeholder="è¯·è¾“å…¥è¦æ·»åŠ å•æ¡æ®µè½çš„å†…å®¹", lines=5)
                one_content_segmentation = gr.Checkbox(value=True, label="ç¦æ­¢å†…å®¹åˆ†å¥å…¥åº“",
                                                                interactive=True)
                mode = gr.Radio([ "mmEngineçŸ¥è¯†åº“é—®ç­”"],
                label="è¯·é€‰æ‹©ä½¿ç”¨æ¨¡å¼",
                value="mmEngineçŸ¥è¯†åº“é—®ç­”", )       
                vs_id='mmEngine'
                vs_path = os.path.join(KB_ROOT_PATH, vs_id, "vector_store")
                files_path='./knowledge_base/mmEngine/content'
                files=[os.path.join(files_path, i) for i in os.listdir(files_path)]
                if not os.path.exists('./knowledge_base/mmEngine/vector_store'):
                    vs_path, loaded_files = local_doc_qa.init_knowledge_vector_store(os.path.join(KB_ROOT_PATH, vs_id, "content"),
                                                                    vs_path, SENTENCE_SIZE)
                # _, loaded_files = local_doc_qa.one_knowledge_add(vs_path, one_conent,  one_conent,one_content_segmentation,
                #                                                 SENTENCE_SIZE)
                    # get_vector_store(vs_path, files, SENTENCE_SIZE, None, None, None)
                vs_path=gr.State(vs_path)
                flag_csv_logger.setup([query, vs_path, chatbot, mode], "flagged")
                    # get_answer(query, vs_path, chatbot, mode,)
                    # query = gr.Textbox(show_label=False,
                    #                placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
                
                query.submit(get_answer,
                    [query, vs_path, chatbot, mode],
                    [chatbot, query])
            
            

# å¯åŠ¨demo
(demo
 .queue(concurrency_count=3)
 .launch(server_name='0.0.0.0',
         server_port=7860,
         show_api=False,
         share=False,
         inbrowser=False))